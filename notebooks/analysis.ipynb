{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code used to make the TesnorBoard. There were issues with other files being imported into the jupyter notebook so there is also a analysis.py file in the notebooks folder that does work all the time. Therefore along with some of the analysis in this file there is this analysis and more are in the report.md file.\n",
    "\n",
    "**All screenshots are in the report.md file along with extra analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TensorboardWriter():\n",
    "    def __init__(self, log_dir, logger, enabled):\n",
    "        self.writer = None\n",
    "        self.selected_module = \"\"\n",
    "\n",
    "        if enabled:\n",
    "            log_dir = str(log_dir)\n",
    "\n",
    "            # Retrieve vizualization writer.\n",
    "            succeeded = False\n",
    "            for module in [\"torch.utils.tensorboard\", \"tensorboardX\"]:\n",
    "                try:\n",
    "                    self.writer = importlib.import_module(module).SummaryWriter(log_dir)\n",
    "                    succeeded = True\n",
    "                    break\n",
    "                except ImportError:\n",
    "                    succeeded = False\n",
    "                self.selected_module = module\n",
    "\n",
    "            if not succeeded:\n",
    "                message = \"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \" \\\n",
    "                    \"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \" \\\n",
    "                    \"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"\n",
    "                logger.warning(message)\n",
    "\n",
    "        self.step = 0\n",
    "        self.mode = ''\n",
    "\n",
    "        self.tb_writer_ftns = {\n",
    "            'add_scalar', 'add_scalars', 'add_image', 'add_images', 'add_audio',\n",
    "            'add_text', 'add_histogram', 'add_pr_curve', 'add_embedding','add_graph'\n",
    "        }\n",
    "        self.tag_mode_exceptions = {'add_graph','add_histogram', 'add_embedding'}\n",
    "        self.timer = datetime.now()\n",
    "\n",
    "    def set_step(self, step, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        if step == 0:\n",
    "            self.timer = datetime.now()\n",
    "        else:\n",
    "            duration = datetime.now() - self.timer\n",
    "            self.add_scalar('steps_per_sec', 1 / duration.total_seconds())\n",
    "            self.timer = datetime.now()\n",
    "\n",
    "    # def add_graph(self, model, input_to_model=None, verbose=False, use_strict_trace=True):\n",
    "        # if self.writer is not None:\n",
    "            # self.writer.add_graph(model, input_to_model, verbose, use_strict_trace)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        If visualization is configured to use:\n",
    "            return add_data() methods of tensorboard with additional information (step, tag) added.\n",
    "        Otherwise:\n",
    "            return a blank function handle that does nothing\n",
    "        \"\"\"\n",
    "        if name in self.tb_writer_ftns:\n",
    "            # Loads the actual function from tensorboard, with the current writer's method as default.\n",
    "            add_data = getattr(self.writer, name, None)\n",
    "\n",
    "            def wrapper(tag, data, *args, **kwargs):\n",
    "                if add_data is not None:\n",
    "                    # add mode(train/valid) tag\n",
    "                    if name not in self.tag_mode_exceptions:\n",
    "                        tag = '{}/{}'.format(tag, self.mode)\n",
    "                    add_data(tag, data, self.step, *args, **kwargs)\n",
    "            return wrapper\n",
    "        else:\n",
    "            # default action for returning methods defined in this class, set_step() for instance.\n",
    "            try:\n",
    "                attr = object.__getattr__(name)\n",
    "            except AttributeError:\n",
    "                raise AttributeError(\"type object '{}' has no attribute '{}'\".format(self.selected_module, name))\n",
    "            return attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Metrics:\n",
    "    def __init__(self, tolerance=0.05):\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    # mean squared error loss\n",
    "    def mse_loss(self, predictions, targets):\n",
    "        return torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "    # accuracy\n",
    "    def accuracy(self, predictions, targets):\n",
    "        correct = (torch.abs(predictions - targets) < self.tolerance).float()\n",
    "        return correct.mean()\n",
    "\n",
    "    # log metrics (epoch, loss, accuracy)\n",
    "    # will print training and validation loss and training and validation accuracy\n",
    "    def log(self, epoch, train_loss, val_loss, train_acc=None, val_acc=None):\n",
    "        print_msg = f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
    "        if train_acc is not None and val_acc is not None:\n",
    "            print_msg += f\" | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        print(print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('/Users/sydneymarder/Desktop/homework #8')\n",
    "\n",
    "from model.metric import Metrics\n",
    "from model.dynamic_model import DenseModel\n",
    "from data_loader.function_dataset import FunctionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load configurations\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load 4 different configurations for different experiments\n",
    "configs = {\n",
    "    \"Basic Configuration\": load_config(\"configs/config.json\"),\n",
    "    \"Optimal Configuration\": load_config(\"configs/optimal.json\"),\n",
    "    \"Overfit Configuration\": load_config(\"configs/overfit.json\"),\n",
    "    \"Underfit Configuration\": load_config(\"configs/underfit.json\")\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # run the experiments\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"Running experiment: {config_name}\")\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    log_dir = f'runs/{config_name.replace(\" \", \"_\")}'\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Initialize model\n",
    "    model = DenseModel(\n",
    "        hidden_layers=config[\"hidden_layers\"],\n",
    "        neurons_per_layer=config[\"neurons_per_layer\"],\n",
    "        activation_hidden=config[\"activation_hidden\"],\n",
    "        activation_output=config[\"activation_output\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize dataset and DataLoader\n",
    "    train_dataset = FunctionDataset(n_samples=1000, function=\"linear\")\n",
    "    val_dataset = FunctionDataset(n_samples=200, function=\"linear\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Log model graph\n",
    "    print(model)\n",
    "    dummy_input = torch.ones(1, 1).to(device)\n",
    "    writer.add_graph(model, dummy_input)\n",
    "\n",
    "    # Initialize metrics and optimizer\n",
    "    metrics = Metrics(tolerance=0.05)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "            loss = metrics.mse_loss(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_accuracy += metrics.accuracy(predictions, y).item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
    "\n",
    "        # Log training metrics\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', avg_train_accuracy, epoch)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_val_accuracy = 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                predictions = model(x)\n",
    "                loss = metrics.mse_loss(predictions, y)\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_accuracy += metrics.accuracy(predictions, y).item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        avg_val_accuracy = total_val_accuracy / len(val_loader)\n",
    "\n",
    "        # Log validation metrics\n",
    "        writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', avg_val_accuracy, epoch)\n",
    "        writer.add_scalar('Time/epoch', time.time() - start_time, epoch)\n",
    "\n",
    "        # Print log\n",
    "        metrics.log(epoch, avg_train_loss, avg_val_loss, avg_train_accuracy, avg_val_accuracy)\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time (Epochs to Convergence)\n",
    "\n",
    "- **Overfit Configuration**:  \n",
    "  - Converges the slowest due to the high model complexity and excessive parameters.  \n",
    "  - Training loss continues to decrease while validation loss starts increasing, indicating memorization rather than learning.  \n",
    "  - The loss curve suggests prolonged optimization with little improvement in generalization.  \n",
    "  - While the model achieves high accuracy on the training set, its performance on validation data deteriorates.  \n",
    "\n",
    "- **Optimal Configuration**:  \n",
    "  - Balances training speed and generalization, converging efficiently within a reasonable number of epochs.  \n",
    "  - The model avoids unnecessary parameter tuning and instead focuses on meaningful feature extraction.  \n",
    "  - Training and validation losses stabilize at a good point, preventing both overfitting and underfitting.  \n",
    "  - This configuration is ideal for deployment as it maximizes accuracy while maintaining computational efficiency.  \n",
    "\n",
    "- **Underfit Configuration**:  \n",
    "  - Converges very quickly but lacks the capacity to learn complex patterns.  \n",
    "  - The model reaches a plateau early on, failing to improve due to limited parameters or insufficient training.  \n",
    "  - Training and validation losses remain high, indicating that the model is unable to capture the underlying structure of the data.  \n",
    "  - This configuration results in poor accuracy and should not be used in practice.  \n",
    "\n",
    "- **Basic Configuration**:  \n",
    "  - Shows moderate convergence speed, neither as fast as the underfit model nor as slow as the overfit one.  \n",
    "  - Its effectiveness depends on hyperparameter tuning, as it may require adjustments to optimize learning.  \n",
    "  - Performs reasonably well but does not outperform the optimal configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss Curves\n",
    "\n",
    "- **Overfit Configuration**:  \n",
    "  - The loss curve sharply drops early, followed by continuous fine-tuning, indicating memorization rather than real learning.  \n",
    "  - Training loss remains low, but the validation loss starts diverging, proving the model is overfitting.  \n",
    "\n",
    "- **Optimal Configuration**:  \n",
    "  - Training loss decreases steadily and stabilizes, preventing overfitting while still achieving good accuracy.  \n",
    "  - Validation loss does not diverge significantly, showing that the model generalizes well to unseen data.  \n",
    "\n",
    "- **Underfit Configuration**:  \n",
    "  - The loss curve flattens almost immediately, suggesting the model stops learning early.  \n",
    "  - The inability to reduce loss further signals insufficient model complexity or poor feature extraction.  \n",
    "\n",
    "- **Basic Configuration**:  \n",
    "  - The loss curve shows decent improvement, but further hyperparameter tuning is needed to match the performance of the optimal configuration.  \n",
    "\n",
    "### Validation Loss Curves\n",
    "\n",
    "- **Overfit Configuration**:  \n",
    "  - Validation loss initially decreases but then starts increasing due to overfitting.  \n",
    "  - The model fails to generalize, leading to poor validation performance despite low training loss.  \n",
    "\n",
    "- **Optimal Configuration**:  \n",
    "  - Validation loss decreases and remains stable, showing that the model does not overfit.  \n",
    "  - This behavior suggests the model learns meaningful patterns that generalize well to unseen data.  \n",
    "\n",
    "- **Underfit Configuration**:  \n",
    "  - Validation loss remains high, confirming the model's inability to extract useful information from the data.  \n",
    "  - This model is not suitable for real-world use as it does not perform well even on training data.  \n",
    "\n",
    "- **Basic Configuration**:  \n",
    "  - The validation loss curve is more stable than in the overfitting case but requires further tuning to reach optimal generalization.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures using `add_graph`\n",
    "\n",
    "- **Overfit Configuration**:\n",
    "  - A highly complex model with excessive parameters, leading to overfitting.  \n",
    "  - Deeper layers and a large number of neurons allow it to memorize data rather than generalize.  \n",
    "\n",
    "- **Optimal Configuration**:\n",
    "  - A well-balanced architecture with enough capacity to learn meaningful patterns without overfitting.  \n",
    "  - Features sufficient depth and width to capture complex representations while avoiding excessive complexity.  \n",
    "\n",
    "- **Underfit Configuration**:\n",
    "  - A shallow network with insufficient parameters, making it incapable of learning useful patterns.  \n",
    "  - The model lacks the capacity to represent the data adequately, leading to high bias.  \n",
    "\n",
    "- **Basic Configuration**:\n",
    "  - A moderately complex architecture, but not optimized for the dataset.  \n",
    "  - Can be adjusted through hyperparameter tuning to improve performance.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
